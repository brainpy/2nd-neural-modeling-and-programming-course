{"cells":[{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"1B34EF7E6E9842679D7CD0DDEAEDC9D2","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"# Synaptic Plasticity  \n## Generate Pre-synaptic Spike trains  \nTo investigate the synaptic plasticity, we need to first simulate the generation process of pre-synaptic spike trains. Here, we simulate two types of spike train given a constant firing rate $fr$, namely the regular spike trains and Poisson spike trains."},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"AE2C7D73B45A4CDEB7E5F5927D6C0F23","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"# Build up the environment\nimport matplotlib.pyplot as plt\nimport brainpy as bp\nimport brainpy.math as bm\nimport numpy as np","outputs":[],"execution_count":1},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"724358605423430890EF08C2062A529A","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"def Pre_spike_generator(type='Poisson', fr=3., dt=1., T=1000.):\n    assert type in ['Poisson', 'Regular']\n    if type == 'Poisson':\n        t = 0\n        spike_train = np.zeros([int(T / dt), ])\n        while t < T:\n            prob = fr * 1e-3 * dt # the probability for the pre-synaptic neuron to fire a spike within this time window\n            spike = (prob > np.random.rand(1)) #0 represents no spike is fired, 1 otherwise\n            spike_train[int(t / dt)] = spike \n            t += dt\n    if type == 'Regular':\n        period = int(1 / fr * 1e3 / dt) # Calculate the average time period for a spike to be fired\n        spike_train = np.zeros([int(T / dt), ]) \n        spike_train[::period] = 1 # In each time period, there is and only is one spike fired\n    return spike_train\n    \n#Generate a spike train\ndt = 1  # ms time window length \nT = 500  # ms\nfr = 10.  # 1e-3*hz\nspike_train_regular = Pre_spike_generator(type='Regular', fr=fr, dt=dt, T=T)\nspike_train_Poisson = Pre_spike_generator(type='Poisson', fr=fr, dt=dt, T=T)\n# Visualize the spike train\nfig, ax = plt.subplots(2,1,figsize=(10, 6))\nax[0].plot(spike_train_regular,label='Regular spikes')\nax[0].legend()\nax[0].set_ylabel('spike event')\nax[1].plot(spike_train_Poisson,label='Poisson spikes')\nax[1].legend()\nax[1].set_xlabel('time(ms)')\nax[1].set_ylabel('spike event')\nplt.show()\n\n\n# Analyze the spike train\nspike_time = np.where(spike_train_regular == 1)[0]\nprint(spike_time)\nspike_num = np.sum(spike_train_regular)\nspike_frequency = spike_num / T * 1e3\nindex = np.zeros(len(spike_time))\nprint(\"Total spike number: {}, Avearge spike frequency: {} \".format(spike_num, spike_frequency))\n\n","outputs":[{"output_type":"stream","name":"stderr","text":"<ipython-input-2-dd99fbadbd77>:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  spike_train[int(t / dt)] = spike\n"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 2 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/724358605423430890EF08C2062A529A/s3zzqpukdi.png\">"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":"[  0 100 200 300 400]\nTotal spike number: 5.0, Avearge spike frequency: 10.0 \n"}],"execution_count":2},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"717897BD4647494FA8304D16AAD5C73A","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"## Simulate the Post-synaptic current and Post-synaptic potential with a invariant exponential synapse model  \nThe post-synaptic current $I_{syn}$ is determined by the product of the post-synaptic conductance and the membrane potential,  \n$$ I_{syn}(t) = g(t)(V_{post}(t)-E_{syn}) $$  \nwhere $E_{syn}$ represents the reversal potential of the synapse, $V_{post}$ represents the membrane potential of the post synaptic neuron. However, considering the time window of the pre-synaptic spike is really small, during which the variations of the membrane potential $V_{post}$ is too small, so we can neglect the variations of the $V_{post}$ and substitute it with the resting membrane potential $V_{post\\_rest}$. In such way, we can simplify the expression of $I_{syn}(t)$  \n\n$$I_{syn}(t) = g(t)(V_{post\\_rest}-E_{syn}) $$  \nfrom which we see that $I_{syn}(t)$ is only determined by $g(t)$. The dynamics of $g(t)$ is simply modelled as an exponential decay:  \n$$ \\frac{dg(t)}{dt} = -\\frac{g(t)}{\\tau_s} + Ay\\delta(t-t_{sp})$$  \nwhere $A$ represents the synaptic strength (which might reflect the number of contacts between pre-synaptic axons and post-synaptic dendrites), $y$ represents the fraction of active neuro-transmitter.  \n\nBefore studying the short-term plasticity, we first simulate the EPSPs and EPSCs generate by a pre-synaptic Regular spike trains when the synaptic strength is invariant (invariable synaptic transimission):"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"B2F8969325BD44728412F6D931E2C95B","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"\nclass Exponential(bp.synapses.TwoEndConn):\n    def __init__(self, pre, post, conn, g_max=0.02, tau=12., delay_step=2, E=0., syn_type='CUBA',\n                 method='exp_auto', **kwargs):\n        super(Exponential, self).__init__(pre=pre, post=post, conn=conn)\n        # 初始化参数\n        self.tau = tau\n        self.g_max = g_max\n        self.delay_step = delay_step\n        self.E = E\n\n        assert syn_type in ['CUBA', 'COBA']  # current-based or conductance-based\n        self.type = syn_type\n\n        # 获取关于连接的信息\n        self.pre2post = self.conn.require('pre2post')  # 获取从pre到post的连接信息\n\n        # 初始化变量\n        self.g = bm.Variable(bm.zeros(self.post.num))\n        self.delay = bm.LengthDelay(self.pre.spike, delay_step)\n\n        # 定义积分函数\n        self.integral = bp.odeint(self.derivative, method=method)\n\n    def derivative(self, g, t):\n        dgdt = -g / self.tau\n        return dgdt\n\n    def update(self):\n        # 取出延迟了delay_step时间步长的突触前脉冲信号\n        delayed_pre_spike = self.delay(self.delay_step)\n        self.delay.update(self.pre.spike)\n\n        # 根据连接模式计算各个突触后神经元收到的信号强度\n        post_sp = bm.pre2post_event_sum(delayed_pre_spike, self.pre2post, self.post.num, self.g_max)\n        # 突触的电导g的更新包括常规积分和突触前脉冲带来的跃变\n        self.g.value = self.integral(self.g, bp.share['t'], bm.dt) + post_sp\n        # 根据不同模式计算突触后电流\n        if self.type == 'CUBA':\n            self.post.input += self.g * (self.E - (-65.))  # E - V_rest\n        else:\n            self.post.input += self.g * (self.E - self.post.V)\n\n\n\ndef run_expsyn(title, sp_times, run_duration=200., **kwargs):\n    neu1 = bp.neurons.SpikeTimeGroup(1, times=sp_times, indices=[0] * len(sp_times)) \n    # the pre-synaptic neuron is simply modelled as a spiketime group\n    neu2 = bp.neurons.LIF(1) # the post-synaptic neuron is modelled as LIF\n    syn1 = Exponential(neu1, neu2, conn=bp.connect.All2All(), **kwargs) # the synapses between two neurons\n    net = bp.Network(pre=neu1, syn=syn1, post=neu2) # two neurons and one synapse form a most simple network\n    # 运行模拟\n    runner = bp.DSRunner(net, monitors=['pre.spike', 'post.V', 'syn.g', 'post.input'])\n    runner.run(run_duration)\n    # 可视化\n    fig, gs = bp.visualize.get_figure(7, 1, 0.8, 6.)\n\n    ax = fig.add_subplot(gs[0, 0])\n    plt.plot(runner.mon.ts, runner.mon['pre.spike'], label='pre.spike')\n    plt.legend(loc='upper right')\n    plt.title(title)\n    plt.xticks([])\n    ax.spines['top'].set_visible(False)\n    # ax.spines['bottom'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n    ax = fig.add_subplot(gs[1:3, 0])\n    plt.plot(runner.mon.ts, runner.mon['syn.g'], label=r'$g(t)$', color=u'#d62728')\n    plt.legend(loc='upper right')\n    plt.xticks([])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n    ax = fig.add_subplot(gs[3:5, 0])\n    plt.plot(runner.mon.ts, runner.mon['post.input'], label='Post Synaptic Current', color=u'#d62728')\n    plt.legend(loc='upper right')\n    plt.xticks([])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n    ax = fig.add_subplot(gs[5:7, 0])\n    plt.plot(runner.mon.ts, runner.mon['post.V'], label='Post Synaptic Potential')\n    plt.legend(loc='upper right')\n    plt.xlabel(r'$t$ (ms)')\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    plt.show()\nprint(spike_time)\nrun_expsyn(title='Exponential Synapse Model (Current-Based)', sp_times=spike_time, run_duration=T, syn_type='CUBA')","outputs":[{"output_type":"stream","name":"stdout","text":"[  0 100 200 300 400]\n"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8164ef56a15b4d2ab86b3277528fd319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x403.2 with 4 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/B2F8969325BD44728412F6D931E2C95B/s3zzqs7epi.png\">"},"metadata":{"needs_background":"light"}}],"execution_count":3},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"3104D8C2C9BC4F2A9B0124CFDBD605DA","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"From these simulation results, we see that the amplitudes of each EPSCs and EPSPs evoked by all pre-synaptic spikes are invariant, which reflect the invariant synaptic strength."},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"B93919E50F454E53BC0610D65970C412","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"# Section 1: Short-term Plasticity  \n## Simulate three-factor short-term depression model  \nThe dynamics of three-factor short-term depression model is as follows:  \n$$\\frac{dx(t)}{dt}=-\\frac{z(t)}{\\tau_{rec}}-U x(t)\\delta(t-t_{sp})$$  \n$$\\frac{dy(t)}{dt}=-\\frac{y(t)}{\\tau_{in}}+U x(t)\\delta(t-t_{sp})$$  \n$$z(t) = 1-x(t)-y(t)$$  \n$$\\frac{dg(t)}{dt}=-\\frac{g(t)}{\\tau_{s}}+g_{max}y(t)$$  \n### Customize the three-factor short-term depression model based on the dynamical equations:"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"0F979E8E557045C68912A28B0F4F901A","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"class tf_STD(bp.synapses.TwoEndConn):\n    def __init__(self, pre, post, conn, g_max=0.1, U=0.25, tau_rec=200., tau_in=3., tau=8., E=1., delay_step=2,\n                 method='exp_auto', syn_type='CUBA', **kwargs):\n        super(tf_STD, self).__init__(pre=pre, post=post, conn=conn)\n        # 初始化参数\n        self.tau_rec = tau_rec\n        self.tau_in = tau_in\n        self.tau = tau\n        self.U = U\n        self.g_max = g_max\n        self.E = E\n        self.delay_step = delay_step\n        self.type = syn_type\n        # 获取每个连接的突触前神经元pre_ids和突触后神经元post_ids\n        self.pre_ids, self.post_ids = self.conn.require('pre_ids', 'post_ids')\n\n        # 初始化变量\n        num = len(self.pre_ids)\n        self.x = bm.Variable(bm.ones(num))\n        self.y = bm.Variable(bm.zeros(num))\n        self.g = bm.Variable(bm.zeros(num))\n        self.delay = bm.LengthDelay(self.g, delay_step)  # 定义一个处理g的延迟器\n\n        # 定义积分函数\n        self.integral = bp.odeint(method=method, f=self.derivative)\n\n    @property  \n    def derivative(self):\n        dx = lambda x, t, y: (1 - x - y) / self.tau_rec #这里代入了z(t) = 1-x(t)-y(t)\n        dy = lambda y, t: -y / self.tau_in\n        dg = lambda g, t: -g / self.tau\n        return bp.JointEq([dx, dy, dg])\n\n    def update(self):\n        # 更新各个变量\n        syn_sps = bm.pre2syn(self.pre.spike, self.pre_ids)  # 哪些突触前神经元产生了脉冲\n        # 模拟没有spike时x,y,g的自然演化\n        x, y, g = self.integral(self.x, self.y, self.g, bp.share['t'], bm.dt)\n        # 模拟当spike到达时产生的突变\n        x = bm.where(syn_sps, x - self.U * self.x, x)  \n        y = bm.where(syn_sps, y + self.U * self.x, y)  \n        g = bm.where(syn_sps, g + self.g_max * y, g) \n        self.x.value = x\n        self.y.value = y\n        self.g.value = g\n        # 更新延迟器, 将g的计算延迟delay_step的时间步长\n        self.delay.update(self.g)\n        delayed_g = self.delay(self.delay_step)\n        # 计算突触后电流\n        post_g = bm.syn2post(delayed_g, self.post_ids, self.post.num)\n        self.post.input += post_g * (self.E - self.post.V_rest)","outputs":[],"execution_count":4},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"A7DCA6E366374B38B215869EBCFFA783","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"### Run the three-factor STD model"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"4F6F8A4E56B6473EB30AB8AC313E57DF","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"\ndef run_tfstd_model(sp_times, tau_rec=500., tau_in=3., run_duration=200., U=0.25, **kwargs):\n    neu1 = bp.neurons.SpikeTimeGroup(1, times=sp_times, indices=[0] * len(sp_times)) #pre neuron\n    neu2 = bp.neurons.LIF(1) # post neuron\n    syn1 = tf_STD(neu1, neu2, conn=bp.connect.All2All(), tau_rec=tau_rec, tau_in=tau_in, U=U) #synapse \n    net1 = bp.Network(pre=neu1, syn=syn1, post=neu2) # network\n    # 运行模拟\n    runner1 = bp.DSRunner(net1, monitors=['pre.spike', 'post.V', 'syn.g', 'syn.x', 'syn.y', 'post.input'])\n    runner1.run(run_duration)\n\n    # 可视化\n    fig1, ax = plt.subplots(4, 1, figsize=(6, 6))\n    ax[0].plot(runner1.mon.ts, runner1.mon['pre.spike'][:, 0], label='pre.spike')\n    ax[0].spines['right'].set_color('none')\n    ax[0].spines['top'].set_color('none')\n    ax[0].legend()\n    ax[0].set_xlabel(r'$t$ (ms)')\n    ax[0].set_ylabel('Pre Spike')\n\n    x = runner1.mon['syn.x'][:, 0]\n    ax[1].plot(runner1.mon.ts, x, label='x(t)')\n    ax[1].legend()\n    ax[1].set_xlabel(r'$t$ (ms)')\n    ax[1].set_ylabel('x')\n    ax[1].spines['right'].set_color('none')\n    ax[1].spines['top'].set_color('none')\n\n    y = runner1.mon['syn.y'][:, 0]\n    ax[2].plot(runner1.mon.ts, y, label='y(t)')\n    ax[2].legend()\n    ax[2].set_xlabel(r'$t$ (ms)')\n    ax[2].set_ylabel('y')\n    ax[2].spines['right'].set_color('none')\n    ax[2].spines['top'].set_color('none')\n\n    g = runner1.mon['syn.g'][:, 0]\n    ax[3].plot(runner1.mon.ts, g, label='g(t)')\n    ax[3].legend()\n    ax[3].set_xlabel(r'$t$ (ms)')\n    ax[3].set_ylabel('g')\n    ax[3].spines['right'].set_color('none')\n    ax[3].spines['top'].set_color('none')\n    plt.tight_layout()\n    plt.show()\n\nrun_tfstd_model(sp_times=spike_time,run_duration=T)","outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93e4e1a0f4aa4b02a9243b8859c8cd2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x432 with 4 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/4F6F8A4E56B6473EB30AB8AC313E57DF/s3zzqvxuwk.png\">"},"metadata":{"needs_background":"light"}}],"execution_count":5},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E9A86EFB08664E83918612DB803E38E4","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"### Underlying mechanism of STD model  \nSince the time constants of the recover process $\\tau_{rec}$ is much slower than the spike time interval, the amount of available neuro-transmitter keeps decreasing, thus giving rise to the short-term depression phenomenon."},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"934D4C655D664CEEBDEBCD6E4EF3CC79","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"## Simplified STD model  \nBy considering the time constants $\\tau_{in}$ to be much smaller than $\\tau_{rec}$, $y(t)$ can be approximately given by,  \n$$y(t)= U x^-\\delta(t-t_{sp})$$  \nSo the dynamics of the three-factor STD model can be simplified as,  \n$$\\frac{dx(t)}{dt}=\\frac{1-x(t)}{\\tau_{rec}}-U x^-\\delta(t-t_{sp})$$  \n$$\\frac{dg(t)}{dt}=-\\frac{g(t)}{\\tau_{s}}+g_{max}Ux^-\\delta(t-t_{sp})$$  \n### Customize simplified STD model in Brainpy"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"CB91AB82347B4C8CA9B138133D03CB69","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"\nclass STD(bp.synapses.TwoEndConn):\n    def __init__(self, pre, post, conn, g_max=0.1, U=0.25, tau_rec=500., tau=8., E=1., delay_step=2,\n                 method='exp_auto', syn_type='CUBA', **kwargs):\n        super(STD, self).__init__(pre=pre, post=post, conn=conn)\n        # 初始化参数\n        self.tau_rec = tau_rec\n        self.tau = tau\n        self.U = U\n        self.g_max = g_max\n        self.E = E\n        self.delay_step = delay_step\n        self.type = syn_type\n        # 获取每个连接的突触前神经元pre_ids和突触后神经元post_ids\n        self.pre_ids, self.post_ids = self.conn.require('pre_ids', 'post_ids')\n\n        # 初始化变量\n        num = len(self.pre_ids)\n        self.x = bm.Variable(bm.ones(num))\n        self.g = bm.Variable(bm.zeros(num))\n        self.delay = bm.LengthDelay(self.g, delay_step)  # 定义一个处理g的延迟器\n\n        # 定义积分函数\n        self.integral = bp.odeint(method=method, f=self.derivative)\n\n    @property \n    def derivative(self):\n        dx = lambda x, t: (1 - x) / self.tau_rec\n        dg = lambda g, t: -g / self.tau\n        return bp.JointEq([dx, dg])\n\n    def update(self):\n        # 更新各个变量\n        syn_sps = bm.pre2syn(self.pre.spike, self.pre_ids)  # 哪些突触前神经元产生了脉冲\n        x, g = self.integral(self.x, self.g, bp.share['t'], bm.dt)\n        x = bm.where(syn_sps, x - self.U * self.x, x)  # 更新后的x\n        g = bm.where(syn_sps, g + self.g_max * self.U * self.x, g)  # 更新后的g\n        self.x.value = x\n        self.g.value = g\n        # 更新延迟器，将g的计算延迟delay_step的时间步长\n        self.delay.update(self.g)\n        delayed_g = self.delay(self.delay_step)\n        # 计算突触后电流\n        post_g = bm.syn2post(delayed_g, self.post_ids, self.post.num)\n        self.post.input += post_g * (self.E - self.post.V_rest)\n","outputs":[],"execution_count":6},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E1DA74511A7E4EDC8F68E0CC5DA922AB","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"### Run the simplified STD model and compare the outputs with the three-factor STD model"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"967DF2026087400B8B28437FA0A8DFE4","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"def compare_std_model(sp_times, tau_rec=500., tau_in=3., run_duration=200., U=0.25, **kwargs):\n    # 定义两组突触前神经元和突触后神经元\n    neu11 = bp.neurons.SpikeTimeGroup(1, times=sp_times, indices=[0] * len(sp_times))\n    neu12 = bp.neurons.LIF(1)\n    neu21 = bp.neurons.SpikeTimeGroup(1, times=sp_times, indices=[0] * len(sp_times))\n    neu22 = bp.neurons.LIF(1)\n    # 分别定义两个synapses，分别是three-factor STD和Simplified STD\n    syn1 = tf_STD(neu11, neu12, conn=bp.connect.All2All(), tau_rec=tau_rec, tau_in=tau_in, U=U)\n    syn2 = STD(neu21, neu22, conn=bp.connect.All2All(), tau_rec=tau_rec, U=U)\n    # 构建两个网络\n    net1 = bp.Network(pre=neu11, syn=syn1, post=neu12)\n    net2 = bp.Network(pre=neu21, syn=syn2, post=neu22)\n\n    # 分别运行模拟\n    runner1 = bp.DSRunner(net1, monitors=['pre.spike', 'post.V', 'syn.g', 'syn.x', 'syn.y', 'post.input'])\n    runner1.run(run_duration)\n\n    runner2 = bp.DSRunner(net2, monitors=['pre.spike', 'post.V', 'syn.g', 'syn.x', 'post.input'])\n    runner2.run(run_duration)\n\n    # 可视化\n    fig, ax = plt.subplots(3, 1, figsize=(6, 6))\n    ax[0].plot(runner1.mon.ts, runner1.mon['pre.spike'][:, 0], label='pre.spike')\n    ax[0].legend()\n    ax[0].set_xlabel(r'$t$ (ms)')\n    ax[0].set_ylabel('spike')\n    ax[0].spines['right'].set_color('none')\n    ax[0].spines['top'].set_color('none')\n    x1 = runner1.mon['syn.x'][:, 0]\n    x2 = runner2.mon['syn.x'][:, 0]\n    ax[1].plot(runner1.mon.ts, x1, label='three factor model')\n    ax[1].plot(runner2.mon.ts, x2, label='simplified model')\n    ax[1].legend()\n    ax[1].set_xlabel(r'$t$ (ms)')\n    ax[1].set_ylabel('x')\n    ax[1].spines['right'].set_color('none')\n    ax[1].spines['top'].set_color('none')\n    g1 = runner1.mon['syn.g'][:, 0]\n    g2 = runner2.mon['syn.g'][:, 0]\n    ax[2].plot(runner1.mon.ts, g1, label='three factor model')\n    ax[2].plot(runner2.mon.ts, g2, label='simplified model')\n    ax[2].legend()\n    ax[2].set_xlabel(r'$t$ (ms)')\n    ax[2].set_ylabel('g')\n    ax[2].spines['right'].set_color('none')\n    ax[2].spines['top'].set_color('none')\n\n    plt.tight_layout()\n    plt.show()\ncompare_std_model(sp_times=spike_time,run_duration=T)","outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c9b4459f3f409b81d3afcb9643d79c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a088cc136a24490c84e015069ae06906"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x432 with 3 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/967DF2026087400B8B28437FA0A8DFE4/s3zzqwsml3.png\">"},"metadata":{"needs_background":"light"}}],"execution_count":7},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E739A3D5A45541CD9BE14A39881FDFD1","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"### Predict post-synaptic conductance with random Poisson spikes  \nIn order to further validate the effectiveness of the model, we need to first infer the parameter values within the model by fitting them to existing data. Then, by simulating the model, we can predict the changes in postsynaptic membrane current and potential when the synapse receives a new random spike train.  \n\nAs an example, we simulate the dynamics of postsynaptic conductance when the presynaptic neuron generates a random Poisson spike train:"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"76F74024B786452E9DD3DF1E30973697","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"spike_time_Poisson = np.where(spike_train_Poisson == 1)[0]\nrun_tfstd_model(sp_times=spike_time_Poisson,run_duration=T)","outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2df3908a49d4dc092cb969bf4e35697"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"D39E9A4F0A2D49968BD34ED484688D9D","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"## Model of Short-term Plasticity  \nBy further considering the dynamics of the release probability $u(t)$, the model is capable of accounting for both the short-term depression and the short-term facilitation phenomena. The dynamics of the model can be written as follows:  \n$$\\frac{du(t)}{dt}=-\\frac{u(t)}{\\tau_{f}}+U (1-u^-) \\delta(t-t_{sp})$$  \n$$\\frac{dx(t)}{dt}=\\frac{1-x(t)}{\\tau_{d}}-u^+x^-\\delta(t-t_{sp})$$  \n$$\\frac{dg(t)}{dt}=-\\frac{g(t)}{\\tau_{s}}+g_{max}u^+x^-\\delta(t-t_{sp})$$  \n### Customize short-term plasticity model in Brainpy"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"A89CB5CD5EAB4F81BB77D07CA9F419F1","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"class STP(bp.synapses.TwoEndConn):\n    def __init__(self, pre, post, conn, g_max=0.1, U=0.15, tau_f=1500., tau_d=200., tau=8., E=1., delay_step=2,\n                 method='exp_auto', syn_type='CUBA', **kwargs):\n        super(STP, self).__init__(pre=pre, post=post, conn=conn)\n        # 初始化参数\n        self.tau_d = tau_d\n        self.tau_f = tau_f\n        self.tau = tau\n        self.U = U\n        self.g_max = g_max\n        self.E = E\n        self.delay_step = delay_step\n        self.type = syn_type\n        # 获取每个连接的突触前神经元pre_ids和突触后神经元post_ids\n        self.pre_ids, self.post_ids = self.conn.require('pre_ids', 'post_ids')\n\n        # 初始化变量\n        num = len(self.pre_ids)\n        self.x = bm.Variable(bm.ones(num))\n        self.u = bm.Variable(bm.zeros(num))\n        self.g = bm.Variable(bm.zeros(num))\n        self.delay = bm.LengthDelay(self.g, delay_step)  # 定义一个处理g的延迟器\n\n        # 定义积分函数\n        self.integral = bp.odeint(method=method, f=self.derivative)\n\n    @property\n    def derivative(self):\n        du = lambda u, t: -u / self.tau_f\n        dx = lambda x, t: (1 - x) / self.tau_d\n        dg = lambda g, t: -g / self.tau\n        return bp.JointEq([du, dx, dg])\n\n    def update(self):\n        # 更新各个变量\n        syn_sps = bm.pre2syn(self.pre.spike, self.pre_ids)  # 哪些突触前神经炎产生了脉冲\n        u, x, g = self.integral(self.u, self.x, self.g, bp.share['t'], bm.dt)\n        u = bm.where(syn_sps, u + self.U * (1 - self.u), u)  # 更新后的u\n        x = bm.where(syn_sps, x - u * self.x, x)  # 更新后的x\n        g = bm.where(syn_sps, g + self.g_max * u * self.x, g)  # 更新后的g\n        self.u.value = u\n        self.x.value = x\n        self.g.value = g\n        # 更新延迟器\n        self.delay.update(self.g)\n        delayed_g = self.delay(self.delay_step)\n        # 计算突触后电流\n        post_g = bm.syn2post(delayed_g, self.post_ids, self.post.num)\n        self.post.input += post_g * (self.E - self.post.V_rest)\n\ndef run_syn_stp(syn_model, title, sp_times, tau_d, tau_f, run_duration=200., **kwargs):\n    neu1 = bp.neurons.SpikeTimeGroup(1, times=sp_times, indices=[0] * len(sp_times))\n    neu2 = bp.neurons.LIF(1)\n    syn1 = syn_model(neu1, neu2, conn=bp.connect.All2All(), tau_d=tau_d, tau_f=tau_f)\n    net = bp.Network(pre=neu1, syn=syn1, post=neu2)\n\n    # 运行模拟\n    runner = bp.DSRunner(net, monitors=['pre.spike', 'post.V', 'syn.g', 'syn.x', 'syn.u', 'post.input'])\n    runner.run(run_duration)\n    # 可视化\n    fig, ax = plt.subplots(2, 2, figsize=(10, 6))\n    ax[0][0].set_title(title)\n    ax[0][0].plot(runner.mon.ts, runner.mon['pre.spike'][:, 0], label='pre.spike')\n    ax[0][0].spines['right'].set_color('none')\n    ax[0][0].spines['top'].set_color('none')\n    ax[0][0].legend()\n    ax[0][0].set_xlabel(r'$t$ (ms)')\n    ax[0][0].set_ylabel('Pre Spike')\n\n    V = runner.mon['post.V'][:, 0] * 10\n    ax[0][1].plot(runner.mon.ts, V, label='Post-Synaptic Potential')\n    ax[0][1].legend()\n    ax[0][1].set_xlabel(r'$t$ (ms)')\n    ax[0][1].set_ylabel('PSP (mV)')\n    ax[0][1].spines['right'].set_color('none')\n    ax[0][1].spines['top'].set_color('none')\n\n    g = runner.mon['syn.g'][:, 0]\n    ax[1][0].plot(runner.mon.ts, g, label='g(t)')\n    ax[1][0].legend()\n    ax[1][0].set_xlabel(r'$t$ (ms)')\n    ax[1][0].set_ylabel('Conductance (mA/mV)')\n    ax[1][0].spines['right'].set_color('none')\n    ax[1][0].spines['top'].set_color('none')\n\n    u = runner.mon['syn.u'][:, 0]\n    x = runner.mon['syn.x'][:, 0]\n    ax[1][1].plot(runner.mon.ts, u, label='u(t)')\n    ax[1][1].plot(runner.mon.ts, x, label='x(t)')\n    ax[1][1].plot(runner.mon.ts, x * u, label='u(t)*x(t)')\n    ax[1][1].legend()\n    ax[1][1].set_xlabel(r'$t$ (ms)')\n    # ax[1].set_ylabel('EPSP (mV)')\n    ax[1][1].spines['right'].set_color('none')\n    ax[1][1].spines['top'].set_color('none')\n    plt.tight_layout()\n    plt.show()","outputs":[],"execution_count":11},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"79E95F61C941445296AD099BEC351CFC","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"### Simulate short-term plasticity by considering slow depression and fast facilitation"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"CEFF3506622D4EA8AA4545FC1E96FAD3","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"tau_d = 500.\ntau_f = 10.\nrun_syn_stp(STP, title='Short term Depression', sp_times=spike_time, run_duration=T, syn_type='CUBA',\n            tau_d=tau_d, tau_f=tau_f)","outputs":[],"execution_count":12},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"74CC62E5C42546928686333F5FCEEFC0","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"### Simulate short-term plasticity by considering fast depression and slow facilitation"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"964D7D7299FA4CCAA08AB07F2C1777FE","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"tau_d = 10.\ntau_f = 500.\nrun_syn_stp(STP, title='Short term Facilitation', sp_times=spike_time, run_duration=T, syn_type='CUBA',\n            tau_d=tau_d, tau_f=tau_f)","outputs":[],"execution_count":13},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"5AFE4B61DA9246B28EDCA740B9A49C92","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"## Rate-based STP model  \nWe investigate the information transmission of STP by studying the time average response of short-term plasticity.  \nBy doing so, the spiking-based STP model is transfered to rate-based STP model, which can be written as,  \n$$\\frac{du(t)}{dt}=-\\frac{u(t)}{\\tau_f}+U[1-u(t)]R(t)$$  \n$$\\frac{dx(t)}{dt}=\\frac{1-x(t)}{\\tau_d}-u^+x(t)R(t)$$  \n$$g(t) = \\tau_s g_{max} u^+ x(t)R(t)$$  \n$$u^+ = u+U(1-u)$$  \n### Customize the rate-based STP model"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"9D99D0CBD9DB46E19B3FE3FD6D8DEE3A","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"class STP_rate(bp.DynamicalSystemNS): \n    def __init__(self, tau_d=500., tau_f=500., U=0.15, tau=8., g_max=0.1, method='exp_auto'):\n        super(STP_rate, self).__init__()\n        # 初始化参数\n        self.tau_d = tau_d\n        self.tau_f = tau_f\n        self.tau = tau\n        self.g_max = g_max\n        self.U = U\n        # 初始化变量\n        self.x = bm.Variable(bm.ones(1))\n        self.u = bm.Variable(bm.zeros(1))\n        self.g = bm.Variable(bm.zeros(1))\n\n        # 定义积分函数\n        self.integral = bp.odeint(method=method, f=self.derivative)\n    @property\n    def derivative(self):\n        du = lambda u, t, input: -u / self.tau_f + self.U * (1 - u) * input #input指的是pre-synaptic firing rate R(t)\n        dx = lambda x, t, u, input: (1 - x) / self.tau_d - (u + self.U * (1 - u)) * x * input\n        return bp.JointEq([du, dx])\n\n    def update(self, input_):\n        # 更新各个变量\n        u, x = self.integral(self.u, self.x, bp.share['t'],  input_, bm.dt)\n        self.u.value = u\n        self.x.value = x\n        self.g.value = self.tau * self.g_max * (u + self.U * (1 - u)) * x * input_","outputs":[],"execution_count":14},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"0775AAAAA4C1491F8D89A07E16060FD9","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"### Compare the simulation results of spiking-based STP model and rate-based STP model  \nTo verify whether the rate-based model replicate the time-average response spiking-based STP model, we compare the simulation results of spiking-based STP and rate-based STP,"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"51F7FF207CA94D41AAC328AE8FDC73F4","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"\ndef compare_spike_rate_Stp(tau_d=500., tau_f=10., U=0.15):\n    T1 = 500\n    T2 = 500\n    T3 = 500\n    fr_1 = 15\n    fr_2 = 30\n    fr_3 = 80\n    # 假设pre-synaptic spike 的频率在0-1000ms是1hz，在1000-2000ms是10hz，在2000-3000ms是20hz，\n    # 模拟spike based stp,\n    spike_1 = Pre_spike_generator(type='Regular', fr=fr_1, T=T1)\n    spike_2 = Pre_spike_generator(type='Regular', fr=fr_2, T=T2)\n    spike_3 = Pre_spike_generator(type='Regular', fr=fr_3, T=T3)\n    spike_train = np.concatenate((spike_1, spike_2, spike_3))\n    spike_time = np.where(spike_train == 1)[0]\n    neu1 = bp.neurons.SpikeTimeGroup(1, times=spike_time, indices=[0] * len(spike_time))\n    neu2 = bp.neurons.LIF(1)\n    syn1 = STP(neu1, neu2, conn=bp.connect.All2All(), tau_d=tau_d, tau_f=tau_f, U=U)\n    net = bp.Network(pre=neu1, syn=syn1, post=neu2)\n    runner_spike = bp.DSRunner(net, monitors=['pre.spike', 'post.V', 'syn.g', 'syn.x', 'syn.u', 'post.input'])\n    runner_spike.run(T1 + T2 + T3)\n    \n    # 模拟rate based stp,\n    dyn_sys = STP_rate(tau_d=tau_d, tau_f=tau_f, U=U)\n    # 分段电流\n    inputs = bp.inputs.section_input(values=[fr_1 * 1e-3, fr_2 * 1e-3, fr_3 * 1e-3],\n                                     durations=[T1, T2, T3])\n    # 运行模拟\n    runner_rate = bp.DSRunner(dyn_sys, monitors=['u', 'x', 'g'])\n    runner_rate.run(inputs=inputs)\n\n    # 可视化\n    fig1, ax = plt.subplots(4, 1, figsize=(6, 6))\n    ax[0].plot(runner_spike.mon.ts, runner_spike.mon['pre.spike'][:, 0], label='pre.spike')\n    ax[0].legend()\n    ax[0].set_ylabel('spike')\n    ax[0].spines['right'].set_color('none')\n    ax[0].spines['top'].set_color('none')\n    x_spike = runner_spike.mon['syn.x'][:, 0]\n    x_rate = runner_rate.mon['x'][:, 0]\n    ax[1].plot(runner_spike.mon.ts, x_spike, label='spike-based model')\n    ax[1].plot(runner_rate.mon.ts, x_rate, label='rate-based model')\n    ax[1].legend()\n    ax[1].set_ylabel('x')\n    ax[1].spines['right'].set_color('none')\n    ax[1].spines['top'].set_color('none')\n    u_spike = runner_spike.mon['syn.u'][:, 0]\n    u_rate = runner_rate.mon['u'][:, 0]\n    ax[2].plot(runner_spike.mon.ts, u_spike, label='spike-based model')\n    ax[2].plot(runner_rate.mon.ts, u_rate, label='rate-based model')\n    ax[2].legend()\n    ax[2].set_ylabel('u')\n    ax[2].spines['right'].set_color('none')\n    ax[2].spines['top'].set_color('none')\n\n    g_spike = runner_spike.mon['syn.g'][:, 0]\n    g_rate = runner_rate.mon['g'][:, 0]\n    ax[3].plot(runner_spike.mon.ts, g_spike, label='spike-based model')\n    ax[3].plot(runner_rate.mon.ts, g_rate, label='rate-based model')\n    ax[3].legend()\n    ax[3].set_xlabel(r'$t$ (ms)')\n    ax[3].set_ylabel('g')\n    ax[3].spines['right'].set_color('none')\n    ax[3].spines['top'].set_color('none')\n    plt.tight_layout()\n    plt.show()\ncompare_spike_rate_Stp()","outputs":[],"execution_count":15},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"944CB9392C234C6587BC573403E18C50","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"# Homework answer of Section 1:  \n# Effects on Information transmission of STP  \n## Frequency-dependent facilitation or depression  \nConsidering the pre-synaptic neuron fires with a constant firing rate $R_0$, by substituting the time derivative $dx/dt$ and $du/dt$ with zero in rate-based STP model, we can solve the theoretical solution of the stationary state of $u(t)$, $x(t)$, $u^+$ and $g(t)$ , which is written as,  \n$$u_{st} = \\frac{UR_0\\tau_f}{1+UR_0\\tau_f}$$  \n$$u_{st}^+ = U\\frac{1+R_0\\tau_f}{1+UR_0\\tau_f}$$  \n$$x_{st} = \\frac{1}{1+u_{st}^+R_0\\tau_d}$$  \n$$EPSC_{st} \\propto g_{st} = g_{max} \\frac{u_{st}^+}{1+u_{st}^+R_0\\tau_d}$$  \n### Simulate the relation between $EPSC_{st}$ and $R_0$"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"5AD7A5268F654BC6AFC28A8B58D7C727","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"def EPSC_st(tau_f, tau_d, R0, U, g_max):\n    u_st = U*(1+R0*tau_f)/(1+U*R0*tau_f)\n    x_st = 1/(1+u_st*R0*tau_f)\n    g_st = g_max*u_st*x_st\n    return g_st\ntau_d=130.\ntau_f=530.\nU=0.03\nn = 100\nR0 = np.linspace(0,100,n)*1e-3\ng_max = 0.1\nEPSC = np.zeros([n,])\nfor i in range(n):\n    EPSC[i] = EPSC_st(tau_f = tau_f,tau_d=tau_d,R0 = R0[i], U=U,g_max=g_max)\nplt.plot(R0*1e3, EPSC, 'b')\nplt.plot(R0*1e3, EPSC, 'r.')\nplt.xlabel('Pre-synaptic firing rate(hz)')\nplt.ylabel('Stationary EPSC')\nplt.show()","outputs":[],"execution_count":16},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E9771349BB724AED8038EAE12183058A","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"From the above results, we can observe that when the presynaptic neuron's firing rate is low, the steady-state EPSC is greater than the initial EPSC (stationary EPSC when the presynaptic firing rate is 0), which means that the synaptic short-term plasticity is primarily dominated as facilitation. However, as the firing rate increases, the steady-state EPSC first increases and then decreases, ultimately becoming smaller than the initial EPSC. This indicates that the short-term facilitation and depression of the synapse are correlated with the firing frequency of the presynaptic neuron.  \n### Simulate the frequency-dependent STF and STD"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"4491BAB34994443A922F0F158E2F5FA8","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"\ndef frequency_dependent_STF(tau_d=130., tau_f=330., U=0.2):\n    T = 1000\n    fr_1 = 10\n    fr_2 = 80\n    # 假设pre-synaptic spike 的频率在0-1000ms是1hz，在1000-2000ms是10hz，在2000-3000ms是20hz，\n    # 模拟spike based stp,\n    spike_1 = Pre_spike_generator(type='Regular', fr=fr_1, T=T)\n    spike_2 = Pre_spike_generator(type='Regular', fr=fr_2, T=T)\n    spike_time_1 = np.where(spike_1 == 1)[0]\n    spike_time_2 = np.where(spike_2 == 1)[0]\n    neu_pre1 = bp.neurons.SpikeTimeGroup(1, times=spike_time_1, indices=[0] * len(spike_time_1))\n    neu_pre2 = bp.neurons.SpikeTimeGroup(1, times=spike_time_2, indices=[0] * len(spike_time_2))\n    neu_post1 = bp.neurons.LIF(1)\n    neu_post2 = bp.neurons.LIF(1)\n    syn1 = STP(neu_pre1, neu_post1, conn=bp.connect.All2All(), tau_d=tau_d, tau_f=tau_f, U=U)\n    syn2 = STP(neu_pre2, neu_post2, conn=bp.connect.All2All(), tau_d=tau_d, tau_f=tau_f, U=U)\n    net1 = bp.Network(pre=neu_pre1, syn=syn1, post=neu_post1)\n    net2 = bp.Network(pre=neu_pre2, syn=syn2, post=neu_post2)\n    runner_1 = bp.DSRunner(net1, monitors=['pre.spike', 'post.V', 'syn.g', 'syn.x', 'syn.u', 'post.input'])\n    runner_1.run(T)\n    runner_2 = bp.DSRunner(net2, monitors=['pre.spike', 'post.V', 'syn.g', 'syn.x', 'syn.u', 'post.input'])\n    runner_2.run(T)\n\n    # 可视化\n    fig1, ax = plt.subplots(2, 2, figsize=(10, 6))\n    ax[0][0].set_title('10hz')\n    ax[0][0].plot(runner_1.mon.ts, runner_1.mon['pre.spike'][:, 0], label='pre.spike')\n    ax[0][0].legend()\n    ax[0][0].set_xlabel(r'$t$ (ms)')\n    ax[0][0].set_ylabel('spike')\n    ax[0][0].spines['right'].set_color('none')\n    ax[0][0].spines['top'].set_color('none')\n    g_1 = runner_1.mon['syn.g'][:, 0]\n    ax[1][0].plot(runner_1.mon.ts, g_1, label='g(t)')\n    ax[1][0].legend()\n    ax[1][0].set_xlabel(r'$t$ (ms)')\n    ax[1][0].set_ylabel('g')\n    ax[1][0].spines['right'].set_color('none')\n    ax[1][0].spines['top'].set_color('none')\n\n    ax[0][1].set_title('80hz')\n    ax[0][1].plot(runner_2.mon.ts, runner_2.mon['pre.spike'][:, 0], label='pre.spike')\n    ax[0][1].legend()\n    ax[0][1].set_xlabel(r'$t$ (ms)')\n    ax[0][1].set_ylabel('spike')\n    ax[0][1].spines['right'].set_color('none')\n    ax[0][1].spines['top'].set_color('none')\n    g_2 = runner_2.mon['syn.g'][:, 0]\n    ax[1][1].plot(runner_2.mon.ts, g_2, label='g(t)')\n    ax[1][1].legend()\n    ax[1][1].set_xlabel(r'$t$ (ms)')\n    ax[1][1].set_ylabel('g')\n    ax[1][1].spines['right'].set_color('none')\n    ax[1][1].spines['top'].set_color('none')\n    plt.show()\nfrequency_dependent_STF()","outputs":[],"execution_count":17},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"7F583B0A5DE3460B9B38D91B376FBBE7","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"The EPSC represents the postsynaptic current generated by each spike from the presynaptic neuron and can indicate the synaptic transmission efficiency - the larger the EPSC, the higher the transmission efficiency. Given the correlation between EPSC and the presynaptic neuron's firing rate, this implies that Short-Term Plasticity (STP) may serve as a synaptic mechanism to implement frequency-dependent gain control.  \n### Simulate the gain control  \nThe synaptic transmission efficiency is related to the presynaptic neuron's firing rate, and there exists an optimal frequency favored by the synapse. We simulate the EPSC of a STP synapse receiving three different pre-synaptic firing rate, and check the frequency-dependent gain control phenomenon."},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"8C6624461A8C47C59D7AEAA37A3E1F07","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"\ndef Gain_control(fr_1=15, fr_2=18, fr_3=35, tau_d=130., tau_f=530., U=0.03):\n    T = 1300\n    fr_1 = fr_1\n    fr_2 = fr_2\n    fr_3 = fr_3\n    spike_1 = Pre_spike_generator(type='Regular', fr=fr_1, T=T)\n    spike_2 = Pre_spike_generator(type='Regular', fr=fr_2, T=T)\n    spike_3 = Pre_spike_generator(type='Regular', fr=fr_3, T=T)\n    spike_time_1 = np.where(spike_1 == 1)[0]\n    spike_time_2 = np.where(spike_2 == 1)[0]\n    spike_time_3 = np.where(spike_3 == 1)[0]\n\n    neu_pre1 = bp.neurons.SpikeTimeGroup(1, times=spike_time_1, indices=[0] * len(spike_time_1))\n    neu_pre2 = bp.neurons.SpikeTimeGroup(1, times=spike_time_2, indices=[0] * len(spike_time_2))\n    neu_pre3 = bp.neurons.SpikeTimeGroup(1, times=spike_time_3, indices=[0] * len(spike_time_3))\n    neu_post1 = bp.neurons.LIF(1)\n    neu_post2 = bp.neurons.LIF(1)\n    neu_post3 = bp.neurons.LIF(1)\n    syn1 = STP(neu_pre1, neu_post1, conn=bp.connect.All2All(), tau_d=tau_d, tau_f=tau_f, U=U)\n    syn2 = STP(neu_pre2, neu_post2, conn=bp.connect.All2All(), tau_d=tau_d, tau_f=tau_f, U=U)\n    syn3 = STP(neu_pre3, neu_post3, conn=bp.connect.All2All(), tau_d=tau_d, tau_f=tau_f, U=U)\n    net1 = bp.Network(pre=neu_pre1, syn=syn1, post=neu_post1)\n    net2 = bp.Network(pre=neu_pre2, syn=syn2, post=neu_post2)\n    net3 = bp.Network(pre=neu_pre3, syn=syn3, post=neu_post3)\n\n    runner_1 = bp.DSRunner(net1, monitors=['pre.spike', 'post.V', 'syn.g', 'syn.x', 'syn.u', 'post.input'])\n    runner_1.run(T)\n    runner_2 = bp.DSRunner(net2, monitors=['pre.spike', 'post.V', 'syn.g', 'syn.x', 'syn.u', 'post.input'])\n    runner_2.run(T)\n    runner_3 = bp.DSRunner(net3, monitors=['pre.spike', 'post.V', 'syn.g', 'syn.x', 'syn.u', 'post.input'])\n    runner_3.run(T)\n\n    # 可视化\n    fig1, ax = plt.subplots(2, 1, figsize=(8, 6))\n    start_time = 10000\n    time = runner_1.mon.ts[start_time:]\n    ax[0].plot(time, runner_1.mon['pre.spike'][start_time:, 0], 'r', label='pre spike train ({}hz)'.format(fr_1))\n    ax[0].plot(time, runner_2.mon['pre.spike'][start_time:, 0], 'g', label='pre spike train ({}hz)'.format(fr_2))\n    ax[0].plot(time, runner_3.mon['pre.spike'][start_time:, 0], 'b', label='pre spike train ({}hz)'.format(fr_3))\n    ax[0].legend()\n    ax[0].set_xlabel(r'$t$ (ms)')\n    ax[0].set_ylabel('spike')\n    ax[0].spines['right'].set_color('none')\n    ax[0].spines['top'].set_color('none')\n\n    g_1 = runner_1.mon['syn.g'][start_time:, 0]\n    g_2 = runner_2.mon['syn.g'][start_time:, 0]\n    g_3 = runner_3.mon['syn.g'][start_time:, 0]\n    ax[1].plot(time, g_1, 'r', label=r'$g_1(t)$')\n    ax[1].plot(time, g_2, 'g', label=r'$g_2(t)$')\n    ax[1].plot(time, g_3, 'b', label=r'$g_3(t)$')\n    ax[1].legend()\n    ax[1].set_xlabel(r'$t$ (ms)')\n    ax[1].set_ylabel('g')\n    ax[1].spines['right'].set_color('none')\n    ax[1].spines['top'].set_color('none')\n    plt.show()\nGain_control()","outputs":[],"execution_count":18},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"3F44E7EF4124457D8F11E6E4C32E1C3B","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"# Section 2: Spike-timing dependent plasticity  \n## Implementation of STDP (proposed by Song, et. al, 2000).  \nGiven a series of pre-synaptic spike timing train ${t_j^n}$ and a series of post-synaptic spike timing train ${t_i^n}$  \n1. One variable trace for the low-pass filtered spike train of the $j_{th}$ presynaptic neuron:  \n$$ \\frac{dx_j}{dt} = -\\frac{x_j}{\\tau_x} + \\sum_{n_j}\\delta(t-t_j^{n_j})$$  \n2. One variable trace for the low-pass filtered spike train of the $i_{th}$ post-synaptic neuron:  \n$$ \\frac{dy_i}{dt} = -\\frac{y_i}{\\tau_y} + \\sum_{n_i}\\delta(t-t_i^{n_i})$$  \n3. The connection weights $w_{ij}$ update rule follows:  \n$$ \\frac{dw_{ij}}{dt} = -F_{-}(w_{ij})y_i(t)\\delta(t-t_j^{n_j})+F_{+}(w_{ij})x_j(t)\\delta(t-t_i^{n_i})$$  \n## Costomize a stdp model in brainpy"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"A5F99E4964134B64AD6A2D84DF59B93D","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"import brainpy as bp\nimport brainpy.math as bm\nimport numpy as np\nimport matplotlib.pyplot as plt\nbm.dt = 0.1\nclass STDPNet(bp.Network):\n   def __init__(self, neu_pre, neu_post):\n     super().__init__()\n     self.pre = neu_pre\n     self.post = neu_post\n    #  self.pre = bp.dyn.LifRef(num_pre, name='neu1')\n    #  self.post = bp.dyn.LifRef(num_post, name='neu2')\n     self.syn = bp.dyn.STDP_Song2000(\n       pre=self.pre,\n       delay=0.,\n       comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(1, pre=self.pre.num, post=self.post.num),\n                                  weight=bp.init.Uniform(max_val=0.1)),\n       syn=bp.dyn.Expon.desc(self.post.varshape, tau=5.),\n       out=bp.dyn.COBA.desc(E=0.),\n       post=self.post, \n       tau_s=16.8,\n       tau_t=33.7,\n       A1=0.96,\n       A2=0.53,\n     )\n    #  self.delay = bm.LengthDelay(self.pre.spike, delay_step=1.)\n\n   def update(self):\n     self.syn()\n     self.pre()\n     self.post()\n     conductance = self.syn.refs['syn'].g\n     Apre = self.syn.refs['pre_trace'].g\n     Apost = self.syn.refs['post_trace'].g\n    #  current = self.post.sum_inputs(self.post.V)\n     return self.pre.spike, self.post.spike, conductance, Apre, Apost, self.syn.comm.weight\n\ndef stdp_run(time_interval, plot=1):\n  duration = 200.\n  pre_spike = bm.array([90])*bm.dt\n  post_spike = pre_spike + time_interval *bm.dt\n  neu_pre = bp.dyn.SpikeTimeGroup(1, times=pre_spike, indices=[0] * len(pre_spike))\n  neu_post = bp.dyn.SpikeTimeGroup(1, times=post_spike, indices=[0] * len(post_spike))\n  # I_pre = bp.inputs.section_input([0, 30, 0, 30, 0, 30, 0, 30, 0, 30, 0, 30, 0],\n  #                                 [5, 15, 15, 15, 15, 15, 100, 15, 15, 15, 15, 15, duration - 255])\n  # I_post = bp.inputs.section_input([0, 30, 0, 30, 0, 30, 0, 30, 0, 30, 0, 30, 0],\n  #                                  [10, 15, 15, 15, 15, 15, 90, 15, 15, 15, 15, 15, duration - 250])\n\n  net = STDPNet(neu_pre=neu_pre, neu_post=neu_post)\n  def run(i):\n    pre_spike, post_spike, g, Apre, Apost, W = net.step_run(i)\n    return pre_spike, post_spike, g, Apre, Apost, W\n\n  indices = bm.arange(0, duration, bm.dt)\n  pre_spike, post_spike, g, Apre, Apost, W = bm.for_loop(run, [indices])\n  if plot == 1:\n    fig, ax = plt.subplots(3,1,figsize = (4,5))\n    ax[0].plot(indices, Apre)\n    ax[0].set_xlabel('time(ms)')\n    ax[0].set_ylabel('pre trace x(t)')\n    ax[1].plot(indices, Apost)\n    ax[1].set_xlabel('time(ms)')\n    ax[1].set_ylabel('post trace y(t)')\n    ax[2].plot(indices, W)\n    ax[2].set_xlabel('time(ms)')\n    ax[2].set_ylabel('conn strength w(t)')\n    plt.tight_layout()\n  # fig, gs = bp.visualize.get_figure(3, 1, 2, 5)\n  # bp.visualize.line_plot(indices, Apre, ax=fig.add_subplot(gs[0, 0]))\n  # bp.visualize.line_plot(indices, Apost, ax=fig.add_subplot(gs[1, 0]))\n  # bp.visualize.line_plot(indices, W, ax=fig.add_subplot(gs[2, 0]))\n  delta_W = W[-1] - W[0]\n  return delta_W\n  \ndelta_W = stdp_run(time_interval=15)\n ","outputs":[],"execution_count":24},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"5A6FAF3A723E4B7FAFC07162126AE328","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"n = 10\ntime_interval = np.linspace(-50,50,n)\ndelta_W = np.zeros(n,)\nfor i in range(n):\n    delta_W[i] = stdp_run(time_interval[i], plot =0)[0]\nplt.plot(-time_interval, delta_W, '.')\nplt.plot(-time_interval[delta_W<=0], delta_W[delta_W<=0], 'b')\nplt.plot(-time_interval[delta_W>0], delta_W[delta_W>0], 'r')\nax = plt.gca()\n# 设置坐标轴的位置\nax.spines['left'].set_position('zero')\nax.spines['right'].set_color('none')\nax.spines['bottom'].set_position('zero')\nax.spines['top'].set_color('none')\n\n# 添加坐标轴标签\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\nplt.tight_layout()\nplt.show()","outputs":[],"execution_count":25},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"AF9A5019656641BE8AC5AF31526B5240","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":""},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"DCAE2A5BAC93415E8C155335CCB9C5CE","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"class STDPNet_lif(bp.Network):\n   def __init__(self, neu_pre, neu_post):\n     super().__init__()\n     self.pre = neu_pre\n     self.post = neu_post\n     self.syn = bp.dyn.STDP_Song2000(\n       pre=self.pre,\n       delay=0.,\n       comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(1, pre=self.pre.num, post=self.post.num),\n                                  weight=bp.init.Uniform(max_val=0.1)),\n       syn=bp.dyn.Expon.desc(self.post.varshape, tau=5.),\n       out=bp.dyn.COBA.desc(E=0.),\n       post=self.post, \n       tau_s=16.8,\n       tau_t=33.7,\n       A1=0.96,\n       A2=0.53,\n     )\n\n   def update(self, I_pre, I_post):\n     self.syn()\n     self.pre(I_pre)\n     self.post(I_post)\n     conductance = self.syn.refs['syn'].g\n     Apre = self.syn.refs['pre_trace'].g\n     Apost = self.syn.refs['post_trace'].g\n    #  current = self.post.sum_inputs(self.post.V)\n     return self.pre.spike, self.post.spike, conductance, Apre, Apost, self.syn.comm.weight\n\ndef stdp_run_lif(time_interval, plot=1):\n  duration = 500.\n  neu_pre = bp.dyn.LifRef(1)\n  neu_post = bp.dyn.LifRef(1)\n  I_pre = bp.inputs.section_input([0, 30, 0, 30, 0, 30, 0, 30, 0, 30, 0, 30, 0],\n                                  [5, 15, 15, 15, 15, 15, 100, 15, 15, 15, 15, 15, duration - 255])\n  I_post = bp.inputs.section_input([0, 30, 0, 30, 0, 30, 0, 30, 0, 30, 0, 30, 0],\n                                   [10, 15, 15, 15, 15, 15, 90, 15, 15, 15, 15, 15, duration - 250])\n\n  net = STDPNet_lif(neu_pre=neu_pre, neu_post=neu_post)\n  def run(i, I_pre, I_post):\n    pre_spike, post_spike, g, Apre, Apost, W = net.step_run(i, I_pre, I_post)\n    return pre_spike, post_spike, g, Apre, Apost, W\n\n  indices = bm.arange(0, duration, bm.dt)\n  pre_spike, post_spike, g, Apre, Apost, W = bm.for_loop(run, [indices, I_pre, I_post])\n  if plot == 1:\n    fig, ax = plt.subplots(3,1,figsize = (4,5))\n    ax[0].plot(indices, Apre)\n    ax[0].set_xlabel('time(ms)')\n    ax[0].set_ylabel('pre trace x(t)')\n    ax[1].plot(indices, Apost)\n    ax[1].set_xlabel('time(ms)')\n    ax[1].set_ylabel('post trace y(t)')\n    ax[2].plot(indices, W)\n    ax[2].set_xlabel('time(ms)')\n    ax[2].set_ylabel('conn strength w(t)')\n    plt.tight_layout()\n  \nstdp_run_lif(time_interval=15)\n ","outputs":[],"execution_count":21},{"cell_type":"markdown","metadata":{"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"A98AEAA0978C4023B0E0E9260265B31D","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"655056aecc918ad4c039c217"},"source":"## Reproduce the following STDP paper:  \n\n- Song, S., Miller, K. & Abbott, L. Competitive Hebbian learning through spike-timing-dependent  \n  synaptic plasticity. Nat Neurosci 3, 919–926 (2000). https://doi.org/10.1038/78829"},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"9010A5DA078B4BF494E22B0FFB169902","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"class STDPNet(bp.DynSysGroup):\n  def __init__(self, num_poisson, num_lif=1, g_max=0.01):\n    super().__init__()\n    self.g_max = g_max\n    # neuron groups\n    self.noise = bp.dyn.PoissonGroup(num_poisson, freqs=15.)\n    self.group = bp.dyn.Lif(num_lif, V_reset=-60., V_rest=-74, V_th=-54, tau=10.,\n                            V_initializer=bp.init.Normal(-60., 1.))\n    # synapses\n    syn = bp.dyn.Expon.desc(num_lif, tau=5.)\n    out = bp.dyn.COBA.desc(E=0.)\n    comm = bp.dnn.AllToAll(num_poisson, num_lif, bp.init.Uniform(0., g_max))\n    self.syn = bp.dyn.STDP_Song2000(self.noise, None, syn, comm, out, self.group,\n                                    tau_s=20, tau_t=20, W_max=g_max, W_min=0.,\n                                    A1=0.01 * g_max, A2=0.0105 * g_max)\n\n  def update(self, *args, **kwargs):\n    self.noise()\n    self.syn()\n    self.group()\n    return self.syn.comm.weight.flatten()[:10]\n\n\ndef run_model():\n  net = STDPNet(1000, 1)\n  indices = np.arange(int(100.0e3 / bm.dt))  # 100 s\n  ws = bm.for_loop(net.step_run, indices, progress_bar=True)\n  weight = bm.as_numpy(net.syn.comm.weight.flatten())\n\n  fig, gs = bp.visualize.get_figure(3, 1, 3, 10)\n  fig.add_subplot(gs[0, 0])\n  plt.plot(weight / net.g_max, '.k')\n  plt.xlabel('Weight / gmax')\n\n  fig.add_subplot(gs[1, 0])\n  plt.hist(weight / net.g_max, 20)\n  plt.xlabel('Weight / gmax')\n\n  fig.add_subplot(gs[2, 0])\n  plt.plot(indices * bm.dt, bm.as_numpy(ws) / net.g_max)\n  plt.xlabel('Time (s)')\n  plt.ylabel('Weight / gmax')\n  plt.show()\n\n\nif __name__ == '__main__':\n  run_model()\n","outputs":[],"execution_count":22},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"4FC27644636541F4AE29CB6A16512C2D","notebookId":"655056aecc918ad4c039c217","trusted":true},"source":"","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":5}